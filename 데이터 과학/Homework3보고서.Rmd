---
title: "Homework 3"
output: html_document
---


```{r}
load("C:/Users/sec/Desktop/교과목들/데이터 과학/bankruptcy.RData")
prs <- read.csv("C:/Users/sec/Desktop/교과목들/데이터 과학/PRSA_data.csv")
```

```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(caret)
library(ROCR)
library(ggplot2)
library(reshape2)
library(corrplot)
library(leaps)
```

## Regression Part

우선 기본 데이터 셋을 확인한다.

```{r}
str(prs)
```

종속변수의 경우 대거 NA값이 포함되어 있으므로 이를 제거하는 과정을 거친다. 그리고 train, test data를 2014년과 그 전 해를 기준으로 구분한다.


```{r}
prs <- prs[!is.na(prs$pm2.5), ]
prs.test <- prs[prs$year == 2014, ]
prs.train <- prs[prs$year != 2014, ]
```

각 데이터셋에 종속변수의 분포를 확인한다. 둘은 유사하여 데이터셋이 잘 구분됨을 알 수 있다.
```{r}
plot(density(prs.train$pm2.5))
```

```{r}
plot(density(prs.test$pm2.5))
```

Q1. 변수들 간의 기본적인 상관관계를 파악해본다. 우선 첫 번쨰로 월별 미세농도의 정도를 파악해본다. 일일이 구하는 방법 이전에 ggplot을 이용하여 년도 단위의, 월별 미세농도 정도를 확인해본다.

```{r}
ggplot(data = prs.train, aes(x = as.factor(month), y = pm2.5)) + geom_boxplot() + facet_wrap(~prs.train$year)
```

전반적으로 가을, 겨울철의 미세먼지 농도가 봄 여름 같이 비교적 따뜻한 날보다는 더 미세먼지 농도가 높았으며,

겨울철에는 미세먼지 농도가 매우 높은 날이 있었음을 확인 -- outlier 존재, 이를 통제하는 것이 관건이 될 것이다.

년도별로 구체적으로 월별 미세먼지 농도 정도를 확인하기 위해 데이터프레임을 쪼개 확인해본다
```{r}
prs.2010 <- prs.train[prs$year == 2010, ]
prs.2011 <- prs.train[prs$year == 2011, ]
prs.2012 <- prs.train[prs$year == 2012, ]
prs.2013 <- prs.train[prs$year == 2013, ]
```

2010년 

2010년의 경우 10~11월이 높게 나타났고 이례적으로 7월도 높게 나타났다.
```{r}
tapply(prs.2010$pm2.5, prs.2010$month, mean)
```

2011년

 2011년의 경우 여전히 10월 중이 높게 나타나고 2월이 가장 높게 나타났다
```{r}
tapply(prs.2011$pm2.5, prs.2011$month, mean)
```

2012년

2012년의 경우 전반적으로 미세먼지 농도의 수치가 낮았으나 12~1월 겨울 중의 미세먼지 농도가 높았다
```{r}
tapply(prs.2012$pm2.5, prs.2012$month, mean)
```

2013년 

2013년의 경우 전반적으로 1~3월 겨울철의 미세먼지 농도가 높았는데 1월의 경우가 압도적으로 미세먼지 농도가 높았다
```{r}
tapply(prs.2013$pm2.5, prs.2013$month, mean)
```


계절과 비슷한 의미를 갖는 온도의 경우도 확인을 해보면
```{r}
ggplot(data = prs.train, aes(x = as.numeric(TEMP), y = pm2.5)) + geom_line()
```

온도의 경우도 뚜렷하지는 않지만 대개 온도가 높을 때 보다는 낮을 때 조금 더 미세먼지 농도가 높은 것 같다.

년도별로도 확인해보자

```{r}
ggplot(data = prs.train, aes(x = as.numeric(TEMP), y = pm2.5)) + geom_line() + facet_wrap(~prs.train$year)
```

2010년과 2011년은 전반적으로 고르게 분포하지만 

2012년과 2013년의 경우 온도가 낮을 수록 더 미세먼지 농도가 많이 분포하는 것처럼 보인다.

tapply를 이용하여 수치를 그래프로 표현하여 확인을 해보면 

```{r}
plot(names(tapply(prs.train$pm2.5, prs.train$TEMP, mean)), tapply(prs.train$pm2.5, prs.train$TEMP, mean))
```

온도가 아주 낮은 -20도 정도를 제외하면 전반적으로 온도가 낮을 때 미세먼지 농도의 정도가 더 큰것처럼 보인다. 하지만 outlier가 많기 때문에 함부로 속단할 수는 없다.

우선 개략적으로

보다 기온이 낮을 때 (그렇다고 낮을수록 무조건 높은 것이 아니라) 우리나라 평균적인 겨울철 온도인 -10~0도 사이의 겨울철과 0~10도의 가을철 온도 때의 경우가 가장 미세먼지 농도가 높았음을 확인할 수 있다

풍향의 경우는 어떠할 까

바람의 방향의 경우 cv일 때 평균적으로 미세 먼지의 농도가 높음을 확인할 수 있다!
```{r}
tapply(prs.train$pm2.5, prs.train$cbwd, mean)
```

그림을 그려 더 정확히 살펴보면
```{r}
ggplot(data = prs.train, aes(x = cbwd, y = pm2.5)) + geom_boxplot() + facet_wrap(~prs.train$year)
```

전반적으로 cv의 경우가 다른 풍향에 비해 중앙값이 크고 더 큰 값인 것처럼 보인다. 하지만 이도 속단할 수는 없다.


풍속의 경우 풍속이 작을 때 미세 먼지의 농도가 높음을 확인할 수 있다! 이는 직관하고 굉장히 비슷한 결과임을 알 수 있다!

```{r}
plot(prs.train$Iws,prs.train$pm2.5) 
```

혹시라도 변수간의 상관관계가 있는 지를 확인한다. 이는 regression 실행 시 다중 공선성의 우려를 막기 위함이다.

상관관계분석이 가능한 이슬점, 온도, 기압, 적설량, 강우량, 풍속을 통해 확인한다.

```{r}
cors <- cor(prs.train[, c(6, 7, 8, 9, 11, 12 ,13)])
corrplot.mixed(cors)
```

분석 결과 이슬점과 온도간 상관관계가 높으며 온도랑 기압, 기압과 이슬점 간의 상관관계가 다소 분포함을 확인할 수 있다. 이는 추후에 vif 다중공선성 검정을 통해 적합 후에 확인하는 것이 다중선형회귀분석 과정에서 필요함을 상기한다

또한 유심히 볼 것인 목표 변수인 pm2.5와 유사한 관계성을 갖는 것이 DEWP변수와 Iws변수이다. 이는 이슬점이랑 풍속을 의미한다.


우선 미세먼지 농도를 예측하는데 년도랑 일수는 중요하지 않은 변수고 계절성을 띠는 월 데이터만 따로 가지고 온다.
```{r}
names(prs.train)
prs.train.adjust <- prs.train[,c(3, 6, 7, 8, 9, 10,11,12,13)]
str(prs.train.adjust)
```

최량 부분집합 회귀분석을 통해 유의미한 변수를 가리는 과정을 밟아본다

```{r}
subset <- regsubsets(pm2.5~., data = prs.train.adjust)
best_summary <- summary(subset)
names(best_summary)
which.min(best_summary$rss)
```

8개의 변수를 선택하는 것이 가장 정확도가 높을 변수인데 그 변수들을 확인해본다.

멜로의 Cp 통계량과 BIC 수치를 가장 낮게 하는 변수를 확인해본다
```{r}
plot(subset, scale = 'Cp')
```

```{r}
plot(subset, scale = 'bic')
```

둘 다 유사하게 DEWP, TEMP, PRES, cbwd(SE제외), lws, ls, lr를 다 선택을 했다

하나의 단일 변수만 선택을 해야하므로 전반적으로 다 비교를 해준다.

저 위의 결과 상 month 변수는 크게 의미가 없는 것처럼 보이지만, 사실 month 데이터는 num으로 되어있기 때문에 비교를 수행해주기로 한다.

Q2.

우선 월 데이터는 정수형으로 1~12로 되어 있지만 사실 이러한 수치는 의미가 없다. 따라서 계절 변수를 형성하여 계절에 따른 미세먼지 농도의 분포를 확인한다.

```{r}
prs.train$season <- cut(prs.train$month, breaks = c(3, 7, 9, 11), right = F, labels = c("spring", "summer", "fall"))
prs.train$season <- ifelse(is.na(prs.train$season), "winter", prs.train$season)
prs.train$season <- as.factor(prs.train$season)
table(prs.train$season)
```

이름을 다시 바꾸어준다

```{r}
prs.train$season <- ifelse(prs.train$season==1, "spring", prs.train$season)
prs.train$season <- ifelse(prs.train$season==2, "summer", prs.train$season)
prs.train$season <- ifelse(prs.train$season==3, "fall", prs.train$season)
prs.train$season <- ifelse(prs.train$season==4, "winter", prs.train$season)
table(prs.train$season)
```

그렇다면 이제 계절별 평균 미세먼지 농도를 확인해본다, 평균 수치를 예측 모형으로 사용한다.

```{r}
tapply(prs.train$pm2.5, prs.train$season, mean)
```

```{r}
sv_models_season <- tapply(prs.train$pm2.5, prs.train$season, mean)

prs.train$prediction_season <- sv_models_season[prs.train$season]
```

error 값을 이용해 RMSE 값을 확인한다.
```{r}
prs.train$error <- prs.train$pm2.5 - prs.train$prediction_season

RMSE <- sqrt(mean(prs.train$error**2))
RMSE     
```

```{r}
sd(prs.train$pm2.5)
```

표준편차랑 비교했을 때 큰차이가 없으므로 예측력이 굉장히 떨어지는 모형임을 확인할 수 있다.

그 다음 온도를 비교해보자

온도의 경우도 수치로 표현이 됬지만, 키, 몸무게와 같이 그냥 회귀를 돌리게 되면 그다지 예측력이 좋지 못하다. 적정한 값으로 grouping을 해주는 과정이 필요하다

```{r}
summary(prs.train$TEMP)
```

기온의 분포는 대강 다음과 같고 대강 이 값에 맞게 grouping을 해준다!

```{r}
prs.train$temp_group <- cut(prs.train$TEMP, breaks = c(-19, 1, 13, 23, 30, Inf), labels = c("very low", "low", "normal", "high", "very high"), right = F)
sv_models_temp <- tapply(prs.train$pm2.5, prs.train$temp_group, mean)
```

마찬가지로 RMSE를 구해본다

```{r}
prs.train$prediction <- sv_models_temp[prs.train$temp_group]
prs.train$error3 <- prs.train$pm2.5 - prs.train$prediction
RMSE
```

오히려 계절 변수보다 더 예측이 좋지 못했다.

온도, 계절 등이 그다지 좋은 변수는 아닌 것 같다. 하지만 온도랑 계절을 나누는 적절한 cutpoint르 잘 못찾아서 그럴 수도 있어서 속단하기는 이르다. 

기압도 확인을 해보자

```{r}
boxplot(prs.train$pm2.5~prs.train$PRES)
```

기압의 경우는 기압이 증가해서 감소하는 과정까지 미세먼지 농도랑 뚜렷한 관계를 찾기는 어려우나 대개 저기압이나 고기압일 때보다는 기압의 수치가
중간의 정도일 때 더 미세먼지가 높은 것처럼 보였다.

```{r}
prs.train$pres_group <- cut(prs.train$PRES, breaks = c(0, 996, 1002, 1008, 1014, 1020, 1026, 1030, 1035, Inf), labels = c("1","2","3","4","5","6","7","8","9"), right = F)
sv_models_pres <- tapply(prs.train$pm2.5, prs.train$pres_group, mean)
prs.train$prediction <- sv_models_pres[prs.train$pres_group]
prs.train$error4 <- prs.train$pm2.5 - prs.train$prediction
RMSE <- sqrt(mean(prs.train$error4**2))
RMSE
```

기압의 경우는 앞서 두 변수에 비해 RMSE 값이 줄기는 했지만 여전히 표준편차와 비교해서 큰 차이가 없다.

풍향은 어떠할 까

```{r}
table(prs.train$cbwd)
```

```{r}
sv_models_cbwd <- tapply(prs.train$pm2.5, prs.train$cbwd, mean)
prs.train$prediction <- sv_models_cbwd[prs.train$cbwd]
prs.train$error5 <- prs.train$pm2.5 - prs.train$prediction
RMSE <- sqrt(mean(prs.train$error5**2))
RMSE
```

풍향의 경우는 조금 더 나은 개선된 값을 보였다.

풍속의 경우는

```{r}
boxplot(prs.train$pm2.5~prs.train$Iws)
```

직관적으로 풍속이 낮을 때 더 미세먼지의 농도가 높은 것처럼 보이나 이는 데이터의 outlier로 인해 발생하는 그래프의 오류일 수도 있다. 우선 저 boxplot 그래프로 나오는 저 녀석들을 cutpoint로 사용하여 group화 한 뒤에 비교를 수행한다.

```{r}
prs.train$wind_group <- cut(prs.train$Iws, breaks = c(0, 1.79, 5.81, 23.26, 25.02, 42, 55, 70.2, 88.5, 111.3, 140.8, 173, 208.3, Inf), labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13"), right = F)
sv_models_wind <- tapply(prs.train$pm2.5, prs.train$wind_group, mean)
prs.train$prediction <- sv_models_wind[prs.train$wind_group]
prs.train$error6 <- prs.train$pm2.5 - prs.train$prediction
RMSE <- sqrt(mean(prs.train$error6**2))
RMSE
```

보다 더 개선된 RMSE가 나왔다.

혹시 모르니 R2값을 구해보면

```{r}
RSS_w <- sum((prs.train$error6)**2)
SST_w <- sum((prs.train$pm2.5 - mean(prs.train$pm2.5))**2)
RSQ_w <- 1 - (RSS_w/SST_w)
RSQ_w
```

0.11 정도의 수치가 나온다. 이는 직접 저 수치로 선형회귀분석을 돌린 R2보다 더 개선된 값을 나타낸다

```{r}
summary(lm(pm2.5~Iws, data=prs.train))
```

마지막으로 이슬점의 경우를 확인해보면

```{r}
boxplot(prs.train$pm2.5~prs.train$DEWP)
```

이슬점의 값이 클 수록 더 미세먼지의 정도가 커짐을 볼 수 있는데 이도 아웃라이어가 많이 분포하니 직접 분석을 해본다

마찬가지로 boxplot에 나온 저 수치를 cutoff로 사용하여 group화한 뒤에 분석을 실행한다

```{r}
prs.train$dew_group <- cut(prs.train$DEWP, breaks = c(-Inf, -28, -23, -18, -13,  -9, -5, -1, 3, 6, 9, 13, 17, 21, Inf), labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14"), right = F)
sv_models_dew <- tapply(prs.train$pm2.5, prs.train$dew_group, mean)
prs.train$prediction <- sv_models_dew[prs.train$dew_group]
prs.train$error7 <- prs.train$pm2.5 - prs.train$prediction
RMSE <- sqrt(mean(prs.train$error7**2))
RMSE
```

앞선 RMSE보다 더 개선된 결과가 나왔다

```{r}
RSS <- sum((prs.train$error7)**2)
SST <- sum((prs.train$pm2.5 - mean(prs.train$pm2.5))**2)
RSQ <- 1 - (RSS/SST)
RSQ
```

R2값도 더 개선됨을 확인할 수 있다. 이 수치 역시 lm의 결과보다 더 개선된 결과이다. 고로 단일 변수로 이 이슬점 변수를 사용하도록 하겠다.

Q3.

위에서 구한 값이 train데이터의 RMSE값과 R2값이다

```{r}
RMSE
RSQ
```
R2 : 0.1128984

RMSE : 86.32859

Q4.
이 모형을 테스트 데이터셋에 적용하여 그 값을 비교하여 본다

```{r}
prs.test$dew_group <- cut(prs.test$DEWP,  breaks = c(-Inf, -28, -23, -18, -13,  -9, -5, -1, 3, 6, 9, 13, 17, 21, Inf), labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14"), right = F)
prs.test$prediction <- sv_models_dew[prs.test$dew_group]
prs.test$error <- prs.test$pm2.5 - prs.test$prediction
RMSE <- sqrt(mean(prs.test$error**2))
RMSE
RSS1 <- sum((prs.test$error)**2)
SST1 <- sum((prs.test$pm2.5-mean(prs.test$pm2.5))**2)
RSQ1 <- 1 - (RSS1/SST1)
RSQ1
```
R2 : 0.1011059

RMSE : 88.67215



조금 그 값이 감소하기는 하였으나 얼추 비슷한 값이 나왔다.

Q5.

과적합은 테스트 데이터에는 굉장히 예측이 좋게 나오나 실제 테스트 데이터나 다른 데이터에서는 그 성능이 맞지 않는, 즉 학습 데이터에 적합이 과하게 되어 나타나는 문제를 말하는 데 이 경우는 학습 데이터와 테스트 데이터의 예측 정확도가 비슷하여 과적합 문제는 나타나지 않는 것으로 보였다.

Q6.
비교는 박성찬(15), 이지현(17) 학우와 같이하였다

1) 우선, 이슬점의 경우가 3명 다 R2값이 크게 나왔었는데 이 중 가장 R2를 발전시키는 방법은 mean보다는 median을 적용하는 방법이 있었다. 특히 박성찬 학우의 이슬점 데이터를 aggregate함수를 통해 각각 값에 구한 median값을 cut을 통해 grouping한 변수는 보다 예측도를 향상시켜 최대 R2를 0.12정도까지 끌어올렸다.

2) 또한 추가적으로 날짜 데이터가 있으므로 시계열 데이터를 적용시킬 수 있어 lubridate를 이용하여 날짜를 맞추어주고 이에 맞게 데이터 프레임을 적용시키는 안을 생각했다. 그 다음, 설명 변수의 변화량에 따른 ((ex) 03:00시때의 이슬점에서 04:00때의 미세먼지 변화량 ) 미세먼지 농도의 변화량을 %로 치환하여 변화량을 추적함으로 이상치를 통제하고자 하는 방안을 생각했었다. 예를 들면

(04:00pm2.5) - (03:00pm2.5) / (03:00pm2.5)

이런식으로 상대적인 변화율을 측정하는 것이다 이후 이 값을 regression을 통해 미세먼지의 변화율을 예측하는 알고리즘을 생각했다.


하지만 이 경우에는 pm2.5가 0이 나오는 경우를 통제할 수가 없었다.

3) 추가적인 방안으로는 시계열 데이터로 이용할 수 있는 부분은 설명 변수의 Time lag를 적용하는 방안이 있었다. 예를 들면 비가 온 데이터는 비가 올 때의 그 때 pm2.5가 아닌 그 1시간 뒤인 pm2.5에 영향을 줄 수도 있다. 이런 식의 Time lag를 적용하는 방법도 추가적으로 사용할 수 있을 것 같았다.




Q7.
이 경우는 train 데이터에서의 산점도이고
```{r}
plot(pm2.5~prediction, data=prs.train, xlab = "pred", ylab = "actual")
```

이 경우는 테스트 데이터에서의 산점도이다.
```{r}
plot(pm2.5~prediction, data=prs.test, xlab = "pred2", ylab = "actual2")
```

우선 그래프만 보면 예측 값이 형편없다 아마 평균값을 기준으로 예측을 잡아서, outlier가 대거 분포해있는 경우 예측력이 몹시 떨어졌다

그래프를 보면 예측 값의 max는 136인데 기본적으로 200이 넘어가는 분포수가 생각보다 많음을 알 수 있다.

grouping을 다시 해주던가 평균이 아닌 다른 값을 사용해야 할 거 같다.


## Classification Part

우선 학습 데이터, 테스트 데이터의 구조를 확인한다.

```{r}
str(bankruptcy_train)

```
```{r}
str(bankruptcy_test)
```

고객의 고유 아이디를 제외하고는 전부 factor형 변수로 치환해준다.

```{r}
bankruptcy_train$Class <- as.factor(bankruptcy_train$Class)
bankruptcy_train$Competitiveness <- as.factor(bankruptcy_train$Competitiveness)
bankruptcy_train$Credibility <- as.factor(bankruptcy_train$Credibility)
bankruptcy_train$`Financial Flexibility` <- as.factor(bankruptcy_train$`Financial Flexibility`)
bankruptcy_train$`Industrial Risk` <- as.factor(bankruptcy_train$`Industrial Risk`)
bankruptcy_train$`Management Risk` <- as.factor(bankruptcy_train$`Management Risk`)
bankruptcy_train$`Operating Risk` <- as.factor(bankruptcy_train$`Operating Risk`)
```

```{r}
bankruptcy_test$Class <- as.factor(bankruptcy_test$Class)
bankruptcy_test$Competitiveness <- as.factor(bankruptcy_test$Competitiveness)
bankruptcy_test$Credibility <- as.factor(bankruptcy_test$Credibility)
bankruptcy_test$`Financial Flexibility` <- as.factor(bankruptcy_test$`Financial Flexibility`)
bankruptcy_test$`Industrial Risk` <- as.factor(bankruptcy_test$`Industrial Risk`)
bankruptcy_test$`Management Risk` <- as.factor(bankruptcy_test$`Management Risk`)
bankruptcy_test$`Operating Risk` <- as.factor(bankruptcy_test$`Operating Risk`)
```

이후 학습 데이터와 테스트 데이터의 종속 변수 분포확인

```{r}
table(bankruptcy_train$Class)
```

```{r}
table(bankruptcy_test$Class)
```

테스트 케이스의 데이터에 비해 다소 학습 데이터에 부도의 경우가 많음이 분포함을 직관적으로 알 수 있다.

우선 corpID는 예측에 그다지 쓰이지 않으므로 제거

```{r}
bankruptcy_trains <- bankruptcy_train[, -1]
bankruptcy_tests <- bankruptcy_test[, -1]
```

Q1.

우선 table 함수를 통해 전반적으로 변수에 따라 부도와 부도가 아닌 기업의 분포 양상을 확인해본다

우선 경쟁력 지표의 경우
```{r}
table(bankruptcy_trains$Competitiveness, bankruptcy_trains$Class)
```

경쟁력은 나름 유의미한 차이를 보이는 것 같다 Positivie와 Negative의 경우는 아예 이분법적으로 구분됨을 확인할 수 있다.

신용성의 경우
```{r}
table(bankruptcy_trains$Credibility, bankruptcy_trains$Class)
```

신용성은 적당히 잘 분포해 있는 것 같다. 

재무 건전성의 경우

```{r}
table(bankruptcy_trains$`Financial Flexibility`, bankruptcy_trains$Class)
```

재무 건전성도 비슷한 유의성을 가지지만 경쟁력이 보다 더 괜찮은 변수처럼 보인다. 

비슷하게 다른 변수들도 개략적인 것을 관찰해보자

산업 리스크의 경우

```{r}
table(bankruptcy_trains$`Industrial Risk`, bankruptcy_trains$Class)
```

경영 리스크의 경우

```{r}
table(bankruptcy_trains$`Management Risk`, bankruptcy_trains$Class)
```

영업 리스크의 경우

```{r}
table(bankruptcy_trains$`Operating Risk`, bankruptcy_trains$Class)
```

리스크 지표들의 경우 앞서 경쟁력, 재무 건전성, 신용성 지표에 비해 구분이 다소 뚜렷하지 못함을 확인할 수 있다.

얼핏 보았을 때는 경쟁력과 재무 건전성이 실제로 부도를 예측하기에 적합한 변수로 판단되나 정확한 분석을 위해 모델링을 하나씩 만들어보기로 한다.

1) 경쟁력 지표
prop.table()을 통해 확률을 확인해보면

```{r}
prop.table(table(bankruptcy_trains$Competitiveness, bankruptcy_trains$Class), margin = 1)
```

실행 결과 경쟁력이 평균 정도의 해당하는 기업은 0.09의 확률로 부도를 겪었다. 

그 말은 평균 정도의 기업이 0.9 정도는 부도를 겪지 않은 것이다.

이 사실을 기반으로 threshold를 0.9 정도로 잡아 0.9보다 확률이 높을 경우 부도를 겪지 않는 것으로 가정한다.


기업들이 부도를 겪지 않음을 예측하기 위해서 모델링을 해보면

prediction이라는 변수를 예측 지표로 삼고

```{r}
sv_mdel_com <- prop.table(table(bankruptcy_trains$Competitiveness, bankruptcy_trains$Class), margin = 1)[,2]
bankruptcy_trains$prob <- sv_mdel_com[bankruptcy_trains$Competitiveness]
bankruptcy_trains$prediction <- bankruptcy_trains$prob > 0.9
bankruptcy_trains$prediction <- ifelse(bankruptcy_trains$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
```

confusionmatrix를 이용해 구분을 해보면
```{r}
tble1 <- table(pred = bankruptcy_trains$prediction, actual = bankruptcy_trains$Class)
tble1
```

4개의 에러를 제외하고 예측이 전반적으로 잘 되었음을 확인할 수 있다.

precision, recall, accuracy에 대한 값을 구해보면
```{r}
acc1 <- sum(diag(tble1))/sum(tble1)
prec1 <- sum(tble1[1,1])/sum(tble1[1,])  
rec1 <- sum(tble1[1,1])/sum(tble1[,1]) 
acc1
prec1
rec1
```

대부분 95% 이상으로 매우 높은 수치를 기록함을 확인할 수 있다.

AUC 결과로 비교를 해야하므로 학습 데이터의 AUC값을 확인해보면 

```{r}
p1 <- prediction(bankruptcy_trains$prob, bankruptcy_trains$Class)
AUC1 <- performance(p1, 'auc')
AUC1@y.values[[1]]
```

0.99로 매우 정확하게 예측함을 알 수가 있다. 하지만 학습 데이터 내의 테스트이므로 과적합의 우려도 생각하고 있어야 한다.

다른 단일 변수를 사용한 예측 모형들의 AUC도 동일한 방법으로 확인을 해보자

2) 신용성

```{r}
prop.table(table(bankruptcy_trains$Credibility, bankruptcy_trains$Class), margin=1)
# 대략 threshold를 0.7로 잡아본다

sv_mdel_cre <- prop.table(table(bankruptcy_trains$Credibility, bankruptcy_trains$Class), margin=1)[, 2]
bankruptcy_trains$prob_c <- sv_mdel_cre[bankruptcy_trains$Credibility]
bankruptcy_trains$prediction <- bankruptcy_trains$prob_c > 0.75
bankruptcy_trains$prediction <- ifelse(bankruptcy_trains$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
tble3 <-  table(pred = bankruptcy_trains$prediction, actual = bankruptcy_trains$Class)
tble3
acc3 <- sum(diag(tble3))/sum(tble3)
prec3 <- sum(tble3[1,1])/sum(tble3[1,])
rec3 <- sum(tble3[1,1])/sum(tble3[,1])
acc3
prec3
rec3
p2 <- prediction(bankruptcy_trains$prob_c, bankruptcy_trains$Class)
AUC2 <- performance(p2, 'auc')
AUC2@y.values
```

신용성의 경우도 AUC가 0.92정도 나와 예측이 어느 정도 잘 된것 같다. 이 경우 threshold를 0.75 정도로 잡았었다.

3) 재무 건전성

```{r}
prop.table(table(bankruptcy_trains$`Financial Flexibility`, bankruptcy_trains$Class), margin=1)
# threshold 0.9
sv_mdel_fin <- prop.table(table(bankruptcy_trains$`Financial Flexibility`, bankruptcy_trains$Class), margin=1)[,2]
bankruptcy_trains$prob_f <- sv_mdel_fin[bankruptcy_trains$`Financial Flexibility`]
bankruptcy_trains$prediction <- bankruptcy_trains$prob_c > 0.9
bankruptcy_trains$prediction <- ifelse(bankruptcy_trains$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
tble4 <-  table(pred = bankruptcy_trains$prediction, actual = bankruptcy_trains$Class)
tble4
acc4 <- sum(diag(tble4))/sum(tble4)
prec4 <- sum(tble4[1,1])/sum(tble4[1,])
rec4 <- sum(tble4[1,1])/sum(tble4[,1])
acc4
prec4
rec4
p3 <- prediction(bankruptcy_trains$prob_f, bankruptcy_trains$Class)
AUC3 <- performance(p3, 'auc')
AUC3@y.values
```
신용성의 경우도 AUC가 0.90정도 나와 예측이 어느 정도 잘 된것 같다. 이 경우 threshold를 0.9 정도로 잡았었다.

4) 산업 리스크

```{r}
prop.table(table(bankruptcy_trains$`Industrial Risk`, bankruptcy_trains$Class), margin=1)
# threshold 0.6
sv_mdel_ind<-prop.table(table(bankruptcy_trains$`Industrial Risk`, bankruptcy_trains$Class), margin=1)[,2]
bankruptcy_trains$prob_i <- sv_mdel_ind[bankruptcy_trains$`Industrial Risk`]
bankruptcy_trains$prediction <- bankruptcy_trains$prob_i > 0.6
bankruptcy_trains$prediction <- ifelse(bankruptcy_trains$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
tble5 <-  table(pred = bankruptcy_trains$prediction, actual = bankruptcy_trains$Class)
tble5
acc5 <- sum(diag(tble5))/sum(tble5)
prec5 <- sum(tble5[1,1])/sum(tble5[1,])
rec5 <- sum(tble5[1,1])/sum(tble5[,1])
acc5
prec5
rec5
p4 <- prediction(bankruptcy_trains$prob_i, bankruptcy_trains$Class)
AUC4 <- performance(p4, 'auc')
AUC4@y.values
```

산업 리스크의 경우 AUC가 0.63정도 나와 그다지 예측이 잘 된것 같지는 않았다. 이 경우 threshold를 0.6 정도로 잡았었다.

5) 경영 리스크

```{r}
prop.table(table(bankruptcy_trains$`Management Risk`, bankruptcy_trains$Class), margin=1)
# threshold 0.6
sv_mdel_man <- prop.table(table(bankruptcy_trains$`Management Risk`, bankruptcy_trains$Class), margin=1)[,2]
bankruptcy_trains$prob_m <- sv_mdel_man[bankruptcy_trains$`Management Risk`]
bankruptcy_trains$prediction <- bankruptcy_trains$prob_m > 0.6
bankruptcy_trains$prediction <- ifelse(bankruptcy_trains$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
tble6 <-  table(pred = bankruptcy_trains$prediction, actual = bankruptcy_trains$Class)
tble6
acc6 <- sum(diag(tble6))/sum(tble6)
prec6 <- sum(tble6[1,1])/sum(tble6[1,])
rec6 <- sum(tble6[1,1])/sum(tble6[,1])
acc6
prec6
rec6
p5 <- prediction(bankruptcy_trains$prob_m, bankruptcy_trains$Class)
AUC5 <- performance(p5, 'auc')
AUC5@y.values
```

경영 리스크의 경우 AUC가 0.70정도 나와 그다지 예측이 잘 된것 같지는 않았다. 이 경우 threshold를 0.6 정도로 잡았었다.

6) 영업 리스크

```{r}
prop.table(table(bankruptcy_trains$`Operating Risk`, bankruptcy_trains$Class), margin=1)
# threshold 0.5
sv_mdel_op <- prop.table(table(bankruptcy_trains$`Operating Risk`, bankruptcy_trains$Class), margin=1)[,2]
bankruptcy_trains$prob_o <- sv_mdel_op[bankruptcy_trains$`Operating Risk`]
bankruptcy_trains$prediction <- bankruptcy_trains$prob_o > 0.5
bankruptcy_trains$prediction <- ifelse(bankruptcy_trains$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
tble7 <-  table(pred = bankruptcy_trains$prediction, actual = bankruptcy_trains$Class)
tble7
acc7 <- sum(diag(tble7))/sum(tble7)
prec7 <- sum(tble7[1,1])/sum(tble7[1,])
rec7 <- sum(tble7[1,1])/sum(tble7[,1]) 
acc7
prec7
rec7
p6 <- prediction(bankruptcy_trains$prob_m, bankruptcy_trains$Class)
AUC6 <- performance(p6, 'auc')
AUC6@y.values
```

영업 리스크의 경우 AUC가 0.70정도 나와 그다지 예측이 잘 된것 같지는 않았다. 이 경우 threshold를 0.5 정도로 잡았었다.

여기서는 train set에서 가장 결과가 좋았던 경쟁력 지표를 단일 변수로 선정한다.

Q2.

경쟁력 지표를 변수로 삼은 모델의 경우 학습 데이터에서의 예측력은 좋았지만 과적합의 우려가 있을 수 있으므로 테스트데이터에서의 확인을 거친다

```{r}
bankruptcy_tests$prob1 <- sv_mdel_com[bankruptcy_tests$Competitiveness]
bankruptcy_tests$prediction <- bankruptcy_tests$prob1 > 0.9
bankruptcy_tests$prediction <- ifelse(bankruptcy_tests$prediction==TRUE,'Non-Bankruptcy', 'Bankruptcy')
tble2 <- table(pred = bankruptcy_tests$prediction, actual = bankruptcy_tests$Class)
tble2
acc2 <- sum(diag(tble2))/sum(tble2)
prec2 <- sum(tble2[1,1])/sum(tble2[1,])  
rec2 <- sum(tble2[1,1])/sum(tble2[,1]) 
acc2
prec2
rec2
final_p <- prediction(bankruptcy_tests$prob1, bankruptcy_tests$Class)
Final_AUC <- performance(final_p, 'auc')
Final_AUC@y.values[[1]]
```

precision, recall, accuracy, AUC 전부 1이 나왔다.

Q3.
precision, recall, accuracy, AUC 전부 1이 나와 모두를 전부 정확하게 예측함을 확인할 수 있다. 이는 과적합의 문제는 없는 것으로 보인다.

Q4.

threshold값이 변할 때 precision, recall값이 변하는 지를 확인하기 위해 performance()함수를 사용한다

```{r}
plot(performance(final_p,'prec'))
```

precision의 경우 threshold(Cutoff)가 0.8과 1.0 사이, 대략 0.9 정도 됬을 때 부터 쭉 값이 1정도가 나옴을 확인할 수 있다.

recall의 경우도
```{r}
plot(performance(final_p, 'rec'))
```

대략적으로  threshold(Cutoff)가 0.8과 1.0 사이, 대략 0.9 정도 됬을때까지 쭉 1을 유지하다 급락하는 것을 확인할 수 있다.

이 둘의 값을 모두 max로 만족시키는 것은 0.9 정도를 threshold로 선택하는 것이 적합한 것처럼 보인다

Q5.

F1 score 점수를 cutoff에 따라 확인해보기 위해 performance()함수를 이용하면
```{r}
plot(performance(final_p, 'f'))
```

이 경우에도 대략 0.9정도 threshold를 잡을 때 가장 F1 score 가 크게 나옴을 확인할 수 있다.

Q6.
앞서 구축했던 best model에 대한 ROC 커브를 그려보면

학습 데이터의 경우
```{r}
plot(performance(p1, 'tpr', 'fpr'))
```

테스트 데이터의 경우
```{r}
plot(performance(final_p, 'tpr','fpr'))
```

우선, 학습 데이터에서의 예측도 훌륭했고, 테스트 데이터의 예측은 전부 다 맞아 떨어졌을 만큼 좋은 예측을 보였다. 

물론 그래프 자체로만 보면 거의 완벽한 데이터처럼 보일 수 있다.

하지만 학습 데이터는 겨우 200개, 테스트 데이터는 50개 뿐이다. 표본의 편향성이 매우 클 수도 있고, 실제로 기업의 부도에 영향을 미치는 변수는 많기에 이 모델이 사회에서 크게 사용될 가능성은 제로일 것이다. 하지만 간단하게라도 시사하는 바는, 경쟁력 지표가 기업의 부도에 있어서 다른 직관적으로 보이는 리스크 관리의 경우보다 어쩌면 더 기업의 부도 여부를 막는 데, 유망 사업에 투자를 하여 회사의 경쟁력을 확보하는 것이 더 용이할 수 있음을 시사할 수 있다.

Q7.
정확도에 대해서도 각각 데이터셋에 대하여 performance()함수를 수행해보면
```{r}
plot(performance(p1, 'acc'))
```
```{r}
plot(performance(final_p, 'acc'))
```

둘다 0.9 정도에서 가장 큰 값이 나옴을 확인할 수 있다.

Q8. 
박성찬(15), 이지현(17)학우랑 같이 Discussion을 수행하였다.

이 경우, 하나의 변수만 가지고 추측을 시도했지만, 다른 변수들을 전부 고려하여 하나의 변수로 만들어주는 작업도 유의미할 것 같았다.

예를들면 각 지표의 Positive, Negative, Average에 대하여 수치값을 적용하여 (ex -1, 0, 1식으루 ) 변수별 합계를 통해 예측을 시도하는 방법도 하나의 좋은 예측 지표일 것 같다는 생각이 들었다.
