# k-Nearest Neighbors (kNN)

k: hyper peramter
k가 너무 많으면 정확도가 떨어진다.
k가 너무 적으면 예외를 확인할 수 없다.


만약 100,000개 중에 Y가 0.1%가 Y일때는 어떻게 해야하는가. 
k가 5000~10000 개정도 되야 yes가 포함될 것이다. 

k는 n의 제곱근이 가장 적절하다고 한다. 


```{r}
install.packages("class")
library(class)

wbcd <- read.csv("https://github.com/hbchoi/SampleData/raw/master/wisc_bc_data.csv", stringsAsFactors = F)
str(wbcd)

# removing ID variable
wbcd <- wbcd[,-1]
table(wbcd$diagnosis)

# changing value for clear interpretation
wbcd$diagnosis <- ifelse(wbcd$diagnosis == 'B', 'Benign', 'Malignant')

summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])

# min-max normalization
minmax_norm <- function(x) {
(x-min(x))/(max(x)-min(x))
}

wbcd_norm <- sapply(wbcd[,-1], minmax_norm)
summary(wbcd_norm[,c("radius_mean", "area_mean", "smoothness_mean")])

# split data into train and test set 
dim(wbcd_norm)

wbcd_train <- wbcd_norm[1:469, ] 
wbcd_test <- wbcd_norm[470:569, ]

wbcd_train_label <- wbcd[1:469, 1] 
wbcd_test_label <- wbcd[470:569, 1]

# choosing proper k 
sqrt(nrow(wbcd_train))

library(class)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_label, k = 21)

#accuracy
mean(wbcd_test_label == wbcd_test_pred) #0.98

#confusion matrix
cmat <- table(wbcd_test_label, wbcd_test_pred)  # 실제, 예측
cmat

#precision
cmat[2,2] / sum(cmat[,2])

#recall
cmat[2,2] / sum(cmat[2,])

# recall 이 더 높아야 하므로 precision을 낮추면 됨. th를 낮춘다. 
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_label, k = 21, prob = TRUE)

head(wbcd_test_pred)
head(attributes(wbcd_test_pred)$prob)

#음성이라면 그대로 확률을 쓸 것이고, 양성이라면 1-prob을 할 것이다 
# converting all Prob to P(Malignant)

wbcd_test_pred_prob <- ifelse(wbcd_test_pred == 'Malignant', 
attributes(wbcd_test_pred)$prob,
1-attributes(wbcd_test_pred)$prob)

head(wbcd_test_pred_prob)

# making Plot

library(ROCR)

plot(performance(prediction(wbcd_test_pred_prob, wbcd_test_label == 'Malignant'), 
'tpr', 'fpr'))

calAUC <- function(predCol, targetCol){
perf <- performance(prediction(predCol, targetCol), 'auc') 
as.numeric(perf@y.values)
}

# AUC for our kNN
calAUC(wbcd_test_pred_prob, wbcd_test_label == 'Malignant')



# set lower threshold 
threshold <- 0.1
wbcd_test_pred_new <- ifelse(wbcd_test_pred_prob > threshold, 
'Malignant', 'Benign')
cmat <- table(wbcd_test_label, wbcd_test_pred_new) 
cmat

#accuracy
mean(wbcd_test_label == wbcd_test_pred_new)

#precision
cmat[2,2] / sum(cmat[,2])

#recall
cmat[2,2] / sum(cmat[2,])



### one hot encoding
sample_df <- data.frame(blood_type = c('A', 'B', 'A','O', 'AB'), 
                        skin_color = c('black', 'white', 'yellow', 'red','black'), 
                        age = c(22, 35, 21, 26, 70))

install.packages("caret")
library(caret)
predict(dummyVars(~ blood_type+skin_color, data = sample_df), sample_df)


```

knn 장점 : 비교적 정확
knn 단점 : 숫자 변수만 된다. 학습데이터가 많아져서 연산량이 많아진다(거리계산을 다 해야 하기 때문)
